{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"cyclegan_tf_ct.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb","timestamp":1590096196640}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v1CUZ0dkOo_F"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"qmkj-80IHxnd","colab":{}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_xnMOsbqHz61"},"source":["# CycleGAN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ds4o1h4WHz9U"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/cyclegan\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e1_Y75QXJS6h"},"source":["## Set up the input pipeline"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5fGHWOKPX4ta"},"source":["Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bJ1ROiQxJ-vY","colab":{}},"source":["!pip install git+https://github.com/tensorflow/examples.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lhSsUx9Nyb3t","colab":{}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YfIk2es3hJEd","colab":{}},"source":["import tensorflow_datasets as tfds\n","from tensorflow_examples.models.pix2pix import pix2pix\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import IPython.display as display\n","from PIL import Image\n","import numpy as np\n","import pathlib\n","\n","tfds.disable_progress_bar()\n","AUTOTUNE = tf.data.experimental.AUTOTUNE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iYn4MdZnKCey"},"source":["## Input Pipeline\n","\n","This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/datasets#cycle_gan). \n","\n","As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n","\n","This is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)\n","\n","* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n","* In random mirroring, the image is randomly flipped horizontally i.e left to right."]},{"cell_type":"code","metadata":{"id":"krSi5Lx0SF_t","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"32xkgT7qSGLg","colab_type":"code","colab":{}},"source":["# data path \n","path_image = '/content/drive/My Drive/Course/CS230/CycleGAN/img_v1'\n","path_label = '/content/drive/My Drive/Course/CS230/CycleGAN/lbl_v5'\n","\n","# data names\n","im_list = os.listdir(path_image)\n","la_list = os.listdir(path_label)\n","\n","# data full path\n","image_list = [os.path.join(path_image, ele) for ele in im_list]\n","label_list = [os.path.join(path_label, ele) for ele in la_list]\n","\n","# manual split\n","image_list_train = image_list[:24]\n","label_list_train = label_list[:30]\n","image_list_test = image_list[24:]\n","label_list_test = label_list[30:]\n","\n","# convert to tensor\n","image_list_train_tf = tf.constant(image_list_train)\n","label_list_train_tf = tf.constant(label_list_train)\n","image_list_test_tf = tf.constant(image_list_test)\n","label_list_test_tf = tf.constant(label_list_test)\n","\n","# print length\n","print(len(image_list_train))\n","print(len(label_list_train))\n","print(len(image_list_test))\n","print(len(label_list_test))\n","\n","print(label_list_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dqpao3Xq-RMt","colab_type":"code","colab":{}},"source":["BUFFER_SIZE = 26\n","BATCH_SIZE = 4\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiIx2QdefyLe","colab_type":"code","colab":{}},"source":["####\n","def random_crop(image):\n","  cropped_image = tf.image.random_crop(image, size=[IMG_WIDTH, IMG_HEIGHT, 3])\n","  return cropped_image\n","\n","####\n","def random_jitter(image):\n","  image = tf.image.resize(image, [256, 256], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","  image = random_crop(image)\n","  image = tf.image.random_flip_left_right(image)\n","  return image\n","\n","####\n","def decode_images(img):\n","  img = tf.image.decode_png(img)\n","  img = tf.image.convert_image_dtype(img, tf.float32)\n","  return img\n","\n","def decode_labels(img):\n","  img = tf.image.decode_png(img, channels=3)\n","  img = tf.image.convert_image_dtype(img, tf.float32)*255.\n","  return img\n","\n","####\n","def process_path_images_train(file_path):\n","  # load the raw data from the file as a string\n","  img = tf.io.read_file(file_path)\n","  img = decode_images(img)\n","  img = random_jitter(img)\n","  return img\n","\n","def process_path_labels_train(file_path):\n","  # load the raw data from the file as a string\n","  img = tf.io.read_file(file_path)\n","  img = decode_labels(img)\n","  img = random_jitter(img)\n","  return img\n","\n","####\n","def process_path_images_test(file_path):\n","  # load the raw data from the file as a string\n","  img = tf.io.read_file(file_path)\n","  img = decode_images(img)\n","  img = random_jitter(img)\n","  return img\n","\n","def process_path_labels_test(file_path):\n","  # load the raw data from the file as a string\n","  img = tf.io.read_file(file_path)\n","  img = decode_labels(img)\n","  img = random_jitter(img)\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKK8heSpfyOu","colab_type":"code","colab":{}},"source":["# global random seed\n","tf.random.set_seed(0)\n","\n","# dataset of string\n","image_train_ds = tf.data.Dataset.from_tensor_slices(image_list_train_tf)\n","label_train_ds = tf.data.Dataset.from_tensor_slices(label_list_train_tf)\n","image_test_ds = tf.data.Dataset.from_tensor_slices(image_list_test_tf)\n","label_test_ds = tf.data.Dataset.from_tensor_slices(label_list_test_tf)\n","\n","# dataset map to arrays\n","image_train_ds = image_train_ds.map(process_path_images_train, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","label_train_ds = label_train_ds.map(process_path_labels_train, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\"\"\"\n","what is the optimal buffer size for test data???\n","\"\"\"\n","image_test_ds = image_test_ds.map(process_path_images_test, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1)\n","label_test_ds = label_test_ds.map(process_path_labels_test, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"myiCdB-unGFe","colab_type":"code","colab":{}},"source":["sample_image = next(iter(image_train_ds))\n","sample_label = next(iter(label_train_ds))\n","print(sample_image.shape)\n","print(sample_label.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fr5cnSw-ws7B","colab_type":"code","colab":{}},"source":["test_sample_image = next(iter(image_test_ds))\n","test_sample_label = next(iter(label_test_ds))\n","print(test_sample_image.shape)\n","print(test_sample_label.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4pOYjMk_KfIB","colab":{}},"source":["plt.figure(figsize=(12,5))\n","plt.subplot(121)\n","plt.title('Image')\n","plt.imshow(sample_image[0])\n","\n","plt.subplot(122)\n","plt.title('Image with random jitter')\n","plt.imshow(random_jitter(sample_image[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0KJyB9ENLb2y","colab":{}},"source":["plt.figure(figsize=(12,5))\n","plt.subplot(121)\n","plt.title('Label')\n","plt.imshow(sample_label[0,:,:,:])\n","\n","plt.subplot(122)\n","plt.title('Label with random jitter')\n","plt.imshow(random_jitter(sample_label[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hvX8sKsfMaio"},"source":["## Import and reuse the Pix2Pix models"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cGrL73uCd-_M"},"source":["Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n","\n","The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n","\n","* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n","* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity.\n","\n","There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n","\n","* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n","* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n","* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n","* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n","\n","![Cyclegan model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cyclegan_model.png?raw=1)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8ju9Wyw87MRW","colab":{}},"source":["generator_g = pix2pix.unet_generator(3, norm_type='instancenorm')\n","generator_f = pix2pix.unet_generator(3, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wDaGZ3WpZUyw","colab":{}},"source":["to_label = generator_g(sample_image)\n","to_image = generator_f(sample_label)\n","plt.figure(figsize=(8, 8))\n","contrast = 8\n","\n","imgs = [sample_image, to_label, sample_label, to_image]\n","title = ['Image', 'To label', 'Label', 'To image']\n","\n","for i in range(len(imgs)):\n","  plt.subplot(2, 2, i+1)\n","  plt.title(title[i])\n","  if i % 2 == 0:\n","    plt.imshow(imgs[i][0])\n","  else:\n","    plt.imshow(imgs[i][0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"O5MhJmxyZiy9","colab":{}},"source":["plt.figure(figsize=(8, 8))\n","\n","plt.subplot(121)\n","plt.title('Is a real label?')\n","plt.imshow(discriminator_y(sample_label)[0, ..., -1], cmap='RdBu_r')\n","\n","plt.subplot(122)\n","plt.title('Is a real image?')\n","plt.imshow(discriminator_x(sample_image)[0, ..., -1], cmap='RdBu_r')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0FMYgY_mPfTi"},"source":["## Loss functions"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JRqt02lupRn8"},"source":["In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n","\n","The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#define_the_loss_functions_and_the_optimizer)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cyhxTuvJyIHV","colab":{}},"source":["LAMBDA = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q1Xbz5OaLj5C","colab":{}},"source":["loss_obj = tf.keras.losses.MeanSquaredError()\n","#loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wkMNfBWlT-PV","colab":{}},"source":["def discriminator_loss(real, generated):\n","  real_loss = loss_obj(tf.ones_like(real), real)\n","\n","  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n","\n","  total_disc_loss = real_loss + generated_loss\n","\n","  return total_disc_loss * 0.5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"90BIcCKcDMxz","colab":{}},"source":["def generator_loss(generated):\n","  return loss_obj(tf.ones_like(generated), generated)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5iIWQzVF7f9e"},"source":["Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n","\n","In cycle consistency loss, \n","\n","* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n","* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n","* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n","\n","$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n","\n","$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n","\n","\n","![Cycle loss](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cycle_loss.png?raw=1)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NMpVGj_sW6Vo","colab":{}},"source":["def calc_cycle_loss(real_image, cycled_image):\n","  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n","  \n","  return LAMBDA * loss1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"U-tJL-fX0Mq7"},"source":["As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n","\n","$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"05ywEH680Aud","colab":{}},"source":["def identity_loss(real_image, same_image):\n","  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n","  return LAMBDA * 0.5 * loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G-vjRM7IffTT"},"source":["Initialize the optimizers for all the generators and the discriminators."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iWCn_PVdEJZ7","colab":{}},"source":["generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","discriminator_x_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n","discriminator_y_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aKUZnDiqQrAh"},"source":["## Checkpoints"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WJnftd5sQsv6","colab":{}},"source":["checkpoint_path = \"drive/My Drive/Course/CS230/CycleGAN/Ching-Ting/checkpoints\"\n","\n","ckpt = tf.train.Checkpoint(generator_g=generator_g,\n","                           generator_f=generator_f,\n","                           discriminator_x=discriminator_x,\n","                           discriminator_y=discriminator_y,\n","                           generator_g_optimizer=generator_g_optimizer,\n","                           generator_f_optimizer=generator_f_optimizer,\n","                           discriminator_x_optimizer=discriminator_x_optimizer,\n","                           discriminator_y_optimizer=discriminator_y_optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Rw1fkAczTQYh"},"source":["## Training\n","\n","Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. Predictions may be less accurate. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NS2GWywBbAWo","colab":{}},"source":["EPOCHS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3XrnFNp4b3eR","colab_type":"code","colab":{}},"source":["show_lst = list(image_train_ds.unbatch().take(10).as_numpy_iterator())\n","print(len(show_lst))\n","print(show_lst[0].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RmdVsmvhPxyy","colab":{}},"source":["\n","# save_path = 'drive/My Drive/Course/CS230/CycleGAN/Ching-Ting/results'\n","# def save_image_list(model, show_lst, epoch=0, save_path=save_path):\n","#   for idx in range(len(show_lst)):\n","#     test_input = np.expand_dims(show_lst[idx], axis=0)\n","#     prediction = model(test_input)\n","    \n","#     plt.ioff()\n","#     fig = plt.figure(figsize=(12, 12))\n","\n","#     display_list = [test_input[0,:,:,:], prediction[0,:,:,:]]\n","#     title = ['Input Image', 'Predicted Image']\n","\n","#     for i in range(2):\n","#       plt.subplot(1, 2, i+1)\n","#       plt.title(title[i])\n","#       # getting the pixel values between [0, 1] to plot it.\n","#       plt.imshow(display_list[i])\n","#       plt.axis('off')\n","    \n","#     plt.savefig(f\"{save_path}/im{idx}_epoch{epoch}.png\")\n","#     plt.close(fig)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJwxASCPPohh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfvS6nnXcsWh","colab_type":"code","colab":{}},"source":["def generate_images(model, test_input):\n","  prediction = model(test_input)\n","    \n","  plt.figure(figsize=(12, 12))\n","\n","  display_list = [test_input[0], prediction[0]]\n","  title = ['Input Image', 'Predicted Image']\n","\n","  for i in range(2):\n","    plt.subplot(1, 2, i+1)\n","    plt.title(title[i])\n","    # getting the pixel values between [0, 1] to plot it.\n","    plt.imshow(display_list[i])\n","    plt.axis('off')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kE47ERn5fyLC"},"source":["Even though the training loop looks complicated, it consists of four basic steps:\n","\n","* Get the predictions.\n","* Calculate the loss.\n","* Calculate the gradients using backpropagation.\n","* Apply the gradients to the optimizer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KBKUV2sKXDbY","colab":{}},"source":["@tf.function\n","def train_step(real_x, real_y):\n","  # persistent is set to True because the tape is used more than\n","  # once to calculate the gradients.\n","  with tf.GradientTape(persistent=True) as tape:\n","    # Generator G translates X -> Y\n","    # Generator F translates Y -> X.\n","    \n","    fake_y = generator_g(real_x, training=True)\n","    cycled_x = generator_f(fake_y, training=True)\n","\n","    fake_x = generator_f(real_y, training=True)\n","    cycled_y = generator_g(fake_x, training=True)\n","\n","    # same_x and same_y are used for identity loss.\n","    same_x = generator_f(real_x, training=True)\n","    same_y = generator_g(real_y, training=True)\n","\n","    disc_real_x = discriminator_x(real_x, training=True)\n","    disc_real_y = discriminator_y(real_y, training=True)\n","\n","    disc_fake_x = discriminator_x(fake_x, training=True)\n","    disc_fake_y = discriminator_y(fake_y, training=True)\n","\n","    # calculate the loss\n","    gen_g_loss = generator_loss(disc_fake_y)\n","    gen_f_loss = generator_loss(disc_fake_x)\n","    \n","    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n","    \n","    # Total generator loss = adversarial loss + cycle loss\n","    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n","    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n","\n","    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n","    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n","  \n","  # Calculate the gradients for generator and discriminator\n","  generator_g_gradients = tape.gradient(total_gen_g_loss, \n","                                        generator_g.trainable_variables)\n","  generator_f_gradients = tape.gradient(total_gen_f_loss, \n","                                        generator_f.trainable_variables)\n","  \n","  discriminator_x_gradients = tape.gradient(disc_x_loss, \n","                                            discriminator_x.trainable_variables)\n","  discriminator_y_gradients = tape.gradient(disc_y_loss, \n","                                            discriminator_y.trainable_variables)\n","  \n","  # Apply the gradients to the optimizer\n","  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n","                                            generator_g.trainable_variables))\n","\n","  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n","                                            generator_f.trainable_variables))\n","  \n","  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n","                                                discriminator_x.trainable_variables))\n","  \n","  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n","                                                discriminator_y.trainable_variables))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vV9VcUh3rivv","colab_type":"code","colab":{}},"source":["## record loss for each epoch\n","Gen_G_Loss = np.zeros((EPOCHS, 1))\n","Gen_F_Loss = np.zeros((EPOCHS, 1))\n","Total_Cycle_Loss = np.zeros((EPOCHS, 1))\n","Total_Gen_G_Loss = np.zeros((EPOCHS, 1))\n","Total_Gen_F_Loss = np.zeros((EPOCHS, 1))\n","Disc_X_Loss = np.zeros((EPOCHS, 1))\n","Disc_Y_Loss = np.zeros((EPOCHS, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-ekswEeQpiq","colab_type":"code","colab":{}},"source":["def save_loss(generator_g, generator_f, discriminator_x, discriminator_y, sample_image, sample_label, epoch):\n","  # compute the loss for every epoch\n","  fake_label = generator_g(sample_image)\n","  cycled_image = generator_f(fake_label)\n","  fake_image = generator_f(sample_label)\n","  cycled_label = generator_g(fake_image)\n","  same_image = generator_f(sample_image)\n","  same_label = generator_g(sample_label)\n","  disc_real_image = discriminator_x(sample_image)\n","  disc_real_label = discriminator_y(sample_label)\n","  disc_fake_image = discriminator_x(fake_image)\n","  disc_fake_label = discriminator_y(fake_label)\n","  # calculate the loss\n","  Gen_G_Loss[epoch] = generator_loss(disc_fake_label)\n","  Gen_F_Loss[epoch] = generator_loss(disc_fake_image)\n","  Total_Cycle_Loss[epoch] = calc_cycle_loss(sample_image, cycled_image) + calc_cycle_loss(sample_label, cycled_label) \n","  # Total generator loss = adversarial loss + cycle loss\n","  Total_Gen_G_Loss[epoch] = Gen_G_Loss[epoch] + Total_Cycle_Loss[epoch] + identity_loss(sample_label, same_label)\n","  Total_Gen_F_Loss[epoch] = Gen_F_Loss[epoch] + Total_Cycle_Loss[epoch] + identity_loss(sample_image, same_image)\n","  Disc_X_Loss[epoch] = discriminator_loss(disc_real_image, disc_fake_image)\n","  Disc_Y_Loss[epoch] = discriminator_loss(disc_real_label, disc_fake_label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2M7LmLtGEMQJ","colab":{}},"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((image_train_ds, label_train_ds)):\n","    train_step(image_x, image_y)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n+=1\n","\n","  clear_output(wait=True)\n","\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_image)\n","  generate_images(generator_f, sample_label)\n","  # if(epoch % 10) == 0:\n","  #   save_image_list(generator_g, show_lst, epoch)\n","  #   save_image_list(generator_f, show_lst, epoch)\n","  \n","  # save_loss(generator_g, generator_f, discriminator_x, discriminator_y, sample_image, sample_label, epoch)\n","\n","  if (epoch + 1) % 5 == 0:\n","    #save_image_list(generator_g, show_lst, epoch)\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LyYo64q7XcP","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"ZGm8zefvmNCd","colab_type":"code","colab":{}},"source":["# data path \n","path_image = '/content/drive/My Drive/Course/CS230/CycleGAN/img_v1'\n","path_label = '/content/drive/My Drive/Course/CS230/CycleGAN/lbl_v1'\n","\n","# data names\n","im_list = os.listdir(path_image)\n","la_list = os.listdir(path_label)\n","\n","# data full path\n","image_list = [os.path.join(path_image, ele) for ele in im_list]\n","label_list = [os.path.join(path_label, ele) for ele in la_list]\n","\n","# manual split\n","image_list_train = image_list[:24]\n","label_list_train = label_list[:60]\n","image_list_test = image_list[24:]\n","label_list_test = label_list[60:]\n","\n","# convert to tensor\n","image_list_train_tf = tf.constant(image_list_train)\n","label_list_train_tf = tf.constant(label_list_train)\n","image_list_test_tf = tf.constant(image_list_test)\n","label_list_test_tf = tf.constant(label_list_test)\n","\n","# print length\n","print(len(image_list_train))\n","print(len(label_list_train))\n","print(len(image_list_test))\n","print(len(label_list_test))\n","\n","print(label_list_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WOD2hS-c29U","colab_type":"code","colab":{}},"source":["image_list_train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-dRi6DPmHSC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZHU2zO_dFP0","colab_type":"code","colab":{}},"source":["label_list_train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVD2Y42EdgeG","colab_type":"code","colab":{}},"source":["image_train_ds = tf.data.Dataset.from_tensor_slices(image_list_train_tf)\n","label_train_ds = tf.data.Dataset.from_tensor_slices(label_list_train_tf)\n","\n","image_train_ds = image_train_ds.map(process_path_images_train, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE)\n","label_train_ds = label_train_ds.map(process_path_labels_train, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPwsD2cZCj1p","colab_type":"code","colab":{}},"source":["i = 0\n","for inp in image_train_ds:\n","  i += 1\n","  prediction = generator_g(inp)\n","  np.save(\"image\"+str(i),inp)\n","  np.save(\"label\"+str(i),prediction)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5m4tw_SHHTH","colab_type":"code","colab":{}},"source":["i = 0\n","for inp in label_train_ds:\n","  i += 1\n","  prediction = generator_f(inp)\n","  np.save(\"shape\"+str(i),inp)\n","  np.save(\"convert\"+str(i),prediction)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQAODzr_IarW","colab_type":"code","colab":{}},"source":["def converge1(img):\n","  \n","  row,col= img.shape\n","  for i in range(row):\n","    for j in range(col):\n","\n","        if(img[i][j]) < 50:\n","          img[i][j] = 0\n","        else:\n","          img[i][j] = 255\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wi1tllBQIs2Y","colab_type":"code","colab":{}},"source":["import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vp0IOYNnJj5T","colab_type":"code","colab":{}},"source":["def normalization(img):\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  row,col= img.shape\n","  img = img * 255\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3E2Qq2aJFAnK","colab_type":"code","colab":{}},"source":["for i in range(1,7):\n","  img = np.load('image'+str(i)+'.npy')\n","  lbl = np.load('label'+str(i)+'.npy')\n","  shape = np.load('shape'+str(i)+'.npy')\n","  convert = np.load('convert'+str(i)+'.npy')\n","  for j in range(4):\n","    label = converge1(normalization(lbl[j]))\n","    truth = normalization(shape[j])\n","    intersection = np.logical_and(label, truth)\n","    union = np.logical_or(label, truth)\n","    iou_score = np.sum(intersection) / np.sum(union)\n","    print(iou_score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVwGB5hJQnrQ","colab_type":"code","colab":{}},"source":["\n","lbl = np.load('label6.npy')\n","label = converge1(normalization(lbl[2]))\n","\n","shape = np.load('shape6.npy')\n","truth = normalization(shape[2])\n","\n","intersection = np.logical_and(label, truth)\n","union = np.logical_or(label, truth)\n","iou_score = np.sum(intersection) / np.sum(union)\n","print(iou_score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxZNkiavRM0T","colab_type":"code","colab":{}},"source":["plt.imshow(label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiCphRjvRZcv","colab_type":"code","colab":{}},"source":["plt.imshow(truth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BueInHMT4_js","colab_type":"code","colab":{}},"source":["\n","plt.figure()\n","plt.plot(np.arange(0,EPOCHS),Gen_G_Loss, label='Gen_G_Loss')\n","plt.plot(np.arange(0,EPOCHS),Gen_F_Loss, label='Gen_F_Loss')\n","plt.legend()\n","\n","plt.figure()\n","plt.plot(np.arange(0,EPOCHS),Total_Cycle_Loss, label='Total_Cycle_Loss')\n","plt.plot(np.arange(0,EPOCHS),Total_Gen_G_Loss, label='Total_Gen_G_Loss')\n","plt.plot(np.arange(0,EPOCHS),Total_Gen_F_Loss, label='Total_Gen_F_Loss')\n","plt.legend()\n","\n","plt.figure()\n","plt.plot(np.arange(0,EPOCHS),Disc_X_Loss, label='Disc_X_Loss')\n","plt.plot(np.arange(0,EPOCHS),Disc_Y_Loss, label='Disc_Y_Loss')\n","plt.legend()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1RGysMU_BZhx"},"source":["## Generate using test dataset"]},{"cell_type":"code","metadata":{"id":"e03E6ZkzM_IH","colab_type":"code","colab":{}},"source":["import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gtSL-lqJPJhT","colab_type":"code","colab":{}},"source":["cd '/content/drive/My Drive/Course/CS230/CycleGAN/Ching-Ting/results'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"blakjaV7Ruqm","colab_type":"code","colab":{}},"source":["def normalization(img):\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  row,col= img.shape\n","  img = img *255\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXwHkk_MMx8y","colab_type":"code","colab":{}},"source":["sample_image = next(iter(image_train_ds))\n","img = normalization(np.float32(sample_image[0]))\n","convert = normalization(np.float32(generator_g(sample_image)[0]))\n","cv2.imwrite(\"img8.png\",img)\n","cv2.imwrite(\"lbl8.png\",convert)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTgSVv3M8ohP","colab_type":"code","colab":{}},"source":["sample_label = next(iter(label_train_ds))\n","img = normalization(np.float32(sample_label[0]))\n","convert = normalization(np.float32(generator_f(sample_label)[0]))\n","cv2.imwrite(\"label6.png\",img)\n","cv2.imwrite(\"SynImg6.png\",convert)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KUgSnmy2nqSP","colab":{}},"source":["# Run the trained model on the test dataset\n","for inp in image_train_ds.take(10):\n","  generate_images(generator_g, inp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hib58fBjBFVs","colab_type":"code","colab":{}},"source":["# Run the trained model on the test dataset\n","for inp in label_train_ds.take(5):\n","  generate_images(generator_f, inp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ABGiHY6fE02b"},"source":["## Next steps\n","\n","This tutorial has shown how to implement CycleGAN starting from the generator and discriminator implemented in the [Pix2Pix](https://www.tensorflow.org/tutorials/generative/pix2pix) tutorial. As a next step, you could try using a different dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/datasets#cycle_gan). \n","\n","You could also train for a larger number of epochs to improve the results, or you could implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator used here."]},{"cell_type":"code","metadata":{"id":"gJyjP5OVzdNQ","colab_type":"code","colab":{}},"source":["fnames = os.listdir(checkpoint_path)\n","print(fnames)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ElnMFPFGqtRF","colab_type":"code","colab":{}},"source":["ckpt_load = tf.train.Checkpoint()\n","status = ckpt_load.restore(f\"{checkpoint_path}/ckpt-52\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFZ9dHcGrDg5","colab_type":"code","colab":{}},"source":["status.assert_consumed()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFufoCRcrFBh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}